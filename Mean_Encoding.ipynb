{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages (write a macro to import all of these at once later)\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sb\n",
    "import h5py\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "import sklearn.model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import catboost as cat\n",
    "\n",
    "# import undersampling methods\n",
    "from imblearn.under_sampling import (ClusterCentroids, RandomUnderSampler,\n",
    "                                     NearMiss, InstanceHardnessThreshold,\n",
    "                                     CondensedNearestNeighbour,\n",
    "                                     EditedNearestNeighbours,\n",
    "                                     RepeatedEditedNearestNeighbours,\n",
    "                                     AllKNN, OneSidedSelection,\n",
    "                                     NeighbourhoodCleaningRule)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "this is an sklearn wrapper for XGBoost. \n",
    "This allows us to use sklearnâ€™s \n",
    "Grid Search with parallel processing \n",
    "in the same way we did for GBM\n",
    "\"\"\"\n",
    "from xgboost.sklearn import XGBClassifier as xgb_sk\n",
    "from xgboost import XGBClassifier as xgb_xgb\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Mean Encoding </center></h1>\n",
    "\n",
    "Mean encoding may be referred to as likelihood encoding or target encoding.\n",
    "\n",
    "Mean encoding produces more information to feed the model and helps the model to work better achieving better loss.\n",
    "If you have a large data set, then this should work fine, if your data set is not too big, then you may want to use other regularizing method on top of this one, like smoothing.\n",
    "\n",
    "You can use stratified K-fold if you want.\n",
    "\n",
    "Finally, there are only two ways, ```mean``` and ```counts```. You can use other ways as well.\n",
    "\n",
    "<font color='red'>**Warning**</font> These function applies changes in place. If you do not want your original dataFrame to change, make a copy of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize_cv(df_tr, cols, target, method, no_folds=5, rand_state=100):\n",
    "    # split the data into no_folds folds\n",
    "    kf = KFold(n_splits=no_folds, shuffle=False, random_state=100)\n",
    "    kf.get_n_splits(df_tr)\n",
    "    \n",
    "    # pick up the target\n",
    "    y_tr = df_tr[target].values\n",
    "    \n",
    "    #\n",
    "    # mean encoding\n",
    "    #\n",
    "    if method=='mean':\n",
    "        for col in cols:\n",
    "            df_tr[col+\"_mean_target\"] = None\n",
    "\n",
    "        for tr_ind, val_ind in kf.split(df_tr):\n",
    "            # gran validation and training sets.\n",
    "            X_tr, X_val = df_tr.iloc[tr_ind].copy(), df_tr.iloc[val_ind].copy()\n",
    "            for col in cols:\n",
    "                \"\"\"\n",
    "                 map mean_encoding of training set back to validation set\n",
    "                 this is done to prevent data leackage.\n",
    "                 This is a way of regularizing (prevent leackage)\n",
    "                 and would work just fine if size of data is big.\n",
    "                \"\"\"\n",
    "                means = X_val[col].map(X_tr.groupby(col)[target].mean())\n",
    "                X_val.loc[:, col+'_mean_target'] = means\n",
    "            df_tr.iloc[val_ind]= X_val\n",
    "\n",
    "        # replace NaNs with global mean\n",
    "        prior = df_tr[target].mean()\n",
    "        df_tr.fillna(prior, inplace=True)\n",
    "        return df_tr\n",
    "    #\n",
    "    # count encoding (Why in the log we were getting division by zero,\n",
    "    #                 while clearly it was not zero?)\n",
    "    elif method=='count':\n",
    "        for col in cols:\n",
    "            df_tr[col+\"_count_target\"] = None\n",
    "\n",
    "        for tr_ind, val_ind in kf.split(df_tr):\n",
    "            # gran validation and training sets.\n",
    "            X_tr, X_val = df_tr.iloc[tr_ind].copy(), df_tr.iloc[val_ind].copy()\n",
    "            for col in cols:\n",
    "                means = X_val[col].map(X_tr.groupby(col)[target].sum())\n",
    "                X_val.loc[:, col+'_count_target'] = means\n",
    "            df_tr.iloc[val_ind]= X_val\n",
    "\n",
    "        # replace NaNs with global count\n",
    "        prior = df_tr[target].count()\n",
    "        df_tr.fillna(prior, inplace=True)\n",
    "        return df_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_train_encode_on_val(train_df, val_df, cols, method, target):\n",
    "    val_df_new = val_df.copy()\n",
    "    if method=='mean':\n",
    "        # pick up the target\n",
    "        y_tr = train_df[target].values\n",
    "\n",
    "        # In the new dataFrame create new columns\n",
    "        # for encodings\n",
    "        for col in cols:\n",
    "            val_df_new[col+\"_mean_target\"] = None\n",
    "\n",
    "        for col in cols:\n",
    "            means = val_df_new[col].map(train_df.groupby(col)[target].mean())\n",
    "            val_df_new.loc[:, col+'_mean_target'] = means\n",
    "\n",
    "        # replace NaNs with global mean\n",
    "        prior = train_df[target].mean()\n",
    "        val_df_new.fillna(prior, inplace=True)\n",
    "        return val_df_new\n",
    "    \n",
    "    elif method=='count':\n",
    "        # pick up the target\n",
    "        y_tr = train_df[target].values\n",
    "\n",
    "        # In the new dataFrame create new columns\n",
    "        # for encodings\n",
    "        for col in cols:\n",
    "            val_df_new[col+\"_count_target\"] = None\n",
    "\n",
    "        for col in cols:\n",
    "            means = val_df_new[col].map(train_df.groupby(col)[target].sum())\n",
    "            val_df_new.loc[:, col+'_count_target'] = means\n",
    "\n",
    "        # replace NaNs with global mean\n",
    "        prior = train_df[target].count()\n",
    "        val_df_new.fillna(prior, inplace=True)\n",
    "        return val_df_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
